# .env.template

# Choose your LLM backend: openai or ollama
LLM_BACKEND=ollama 


# Ollama Configuration
OLLAMA_HOST_URL=http://localhost:11434

# Change this to any other supported Ollama models available locally.
OLLAMA_MODEL=gemma3:1b

